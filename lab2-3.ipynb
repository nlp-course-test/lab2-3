{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS187\n",
    "## Lab 2-1 â€“ Sequence labeling with hidden Markov models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov models (HMM) are a fundamental generative method for sequence labeling NLP tasks such as part-of-speech tagging (as in the present lab) and information extraction (as in the second project segment). In this lab, you'll train, apply, and evaluate some simple sequnce labeling algorithms culminating in HMM.\n",
    "\n",
    "To keep things manageable, the dataset you'll use will involve very few word types, only six (plus a special beginning of sentence token), but these are quite ambiguous with regard to part of speech. We'll use the following codes for parts of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = [\n",
    "    \"<bos>\", # beginning of sentence marker\n",
    "    \"N\",     # noun\n",
    "    \"V\",     # main verb\n",
    "    \"M\",     # modal verb\n",
    "    \"P\",     # preposition\n",
    "    \"A\",     # adjective\n",
    "    \"R\"      # adverb\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary of word types, along with their parts of speech, is given by the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {\n",
    "    \"<bos>\":   [\"<bos>\"],\n",
    "    \"can\":     [\"N\", \"V\", \"M\"],\n",
    "    \"canned\":  [\"A\", \"V\"],\n",
    "    \"canners\": [\"N\"],\n",
    "    \"fish\":    [\"N\", \"V\"],\n",
    "    \"for\":     [\"P\"],\n",
    "    \"not\":     [\"R\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few sentences constructed with these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    <bos> canners canned fish\n",
    "    <bos> can canners can fish\n",
    "    <bos> fish can not fish\n",
    "    <bos> can fish can fish can\n",
    "    <bos> canners fish fish for can\n",
    "    <bos> canners can fish for fish\n",
    "    <bos> canners fish for fish\n",
    "    <bos> fish can canned fish\n",
    "    <bos> canners can not can canned fish\n",
    "    <bos> fish can can fish for canners\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding POS sequences, for the first few sentences. Complete the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "text_pos = \"\"\"\n",
    "    <bos> N V N\n",
    "    <bos> M N V N\n",
    "    <bos> N M R V\n",
    "    <bos> M N V N N\n",
    "    <bos> N V N P N\n",
    "    <bos> N M V P N\n",
    "\n",
    "    ...fill this out for the remaining four sentences...\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "text_pos = \"\"\"\n",
    "    <bos> N V N\n",
    "    <bos> M N V N\n",
    "    <bos> N M R V\n",
    "    <bos> M N V N N\n",
    "    <bos> N V N P N\n",
    "    <bos> N M V P N\n",
    "    <bos> N V P N\n",
    "    <bos> N V A N\n",
    "    <bos> N M R V A N\n",
    "    <bos> N M V N P N\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here is the frequency distribution for each word type and each part of speech it can be used as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconstruct(text):\n",
    "    result = []\n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        result.append([item for item in line.strip().split()])\n",
    "    return result\n",
    "\n",
    "tagged_text = [list(zip(sentence, poses))\n",
    "                 for sentence, poses \n",
    "                   in zip(deconstruct(text), deconstruct(text_pos))]\n",
    "               \n",
    "counts = defaultdict(lambda: defaultdict(int))\n",
    "for sentence in tagged_text:\n",
    "    for type, pos in sentence:\n",
    "        counts[type][pos] += 1\n",
    "\n",
    "for type, type_counts in counts.items():\n",
    "    for pos, count in type_counts.items():\n",
    "        print(f'{type:7} {pos:5} {count:2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority label\n",
    "\n",
    "$$\n",
    "   \\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "   \\newcommand{\\Prob}{{\\Pr}}\n",
    "   \\newcommand{\\given}{\\,|\\,}\n",
    "   \\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "   \\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "$$\n",
    "The first sequence labeling method we'll use is simply to choose for each word the POS label it most frequently occurs as in the training data. The table above provides the required information directly.\n",
    "\n",
    "Choosing the majority label for a word sequence $\\vect{x} = \\langle{x_1, x_2, \\ldots, x_m}\\rangle$ is tantamount to maximizing the probability of the label sequence assuming independence of the label conditioned on the word, that is, selecting the tag sequence $\\vect{t} = \\langle{t_1, t_2, \\ldots, t_m}\\rangle$ given by\n",
    "$$ \\argmax{\\vect{t}} \\prod_{i=1}^m \\Prob(t_i \\given x_i) $$\n",
    "\n",
    "How would the majority label method label the following test sentence (which we've marked with the words' correct parts of speech)?\n",
    "\n",
    "> canners[N] can[V] canned[A] fish[N]\n",
    "\n",
    "Give your answer in the form of a sequence of POS labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "example_majority_label = [\"your\", \"answer\", \"here\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "example_majority_label = [\"<bos\", \"N\", \"M\", \"V\", \"N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection, what is the accuracy of the majority labeling, given as a proportion of the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "example_maj_label_accuracy = \"your answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "example_maj_label_accuracy = 3/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority bigram label\n",
    "\n",
    "It may occur to you that what part of speech a word has depends on the context. Suppose we relax the assumption that tag probabilities depend only on the word being tagged, and condition them on the previous word as well. (For the first word in the sentence, we'll condition on that fact. Equivalently, we'll assume that the word \"prior\" to the first word is a special start token.) In summary, we'll condition on the bigram that ends at the word being tagged:\n",
    "$$ \\argmax{\\vect{t}} \\prod_{i=1}^m \\Prob(t_i \\given x_{i-1} x_i) $$\n",
    "\n",
    "What tag sequence is obtained by using the majority bigram labels for the same sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "example_majority_bigram_label = [\"your\", \"answer\", \"here\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "example_majority_bigram_label = [\"<bos>\", \"N\", \"M\", \"A\", \"N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection, what is the accuracy of the majority bigram labeling, given as a proportion of the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "example_maj_bigram_label_accuracy = \"your answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "example_maj_bigram_label_accuracy = 4/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov models\n",
    "\n",
    "Now we get to the real point, using an HMM model. Recall that in an HMM model, we assume that the joint tag/word sequence is generated by \n",
    "\n",
    "1. Selecting a tag sequence according to a Markov model whose states correspond to tags and whose transitions from state $t_i$ to $t_j$ are governed by a _transition probability_ $a_{ij} = \\Prob(t_i \\rightarrow t_j)$, and then\n",
    "2. Selecting a word sequence from the tag sequence where for tag $t_i$ we observe word $x_i$ of type $w_j$ governed by an _emission probability_ $b_{i}(w_j) = \\Prob(t_i \\rightarrow w_j)$.\n",
    "\n",
    "### Estimating the transition and emission probabilities\n",
    "\n",
    "We estimate these transition and emission probabilities by looking at the empirical probabilities in the training data, counting and perhaps smoothing as usual. That is, for the (unsmoothed) transition probabilities, we estimate\n",
    "$$ a_{ij} \\approx \\frac{\\cnt{t_i \\rightarrow t_j}}{\\cnt{t_i}} $$\n",
    "and for the emission probabilities\n",
    "$$ b_i(w_j) \\approx \\frac{\\cnt{t_i \\rightarrow w_j}}{\\cnt{t_i}} $$\n",
    "\n",
    "For instance, we note that there are 4 times in the training data where the tag $N$ is followed by the tag $M$, out of the 21 occurrences of the tag $N$. Thus, we estimate the corresponding transition probability $a_{NM} \\approx 4/21$.\n",
    "\n",
    "\n",
    "Similarly, the emission probability $b_M(can)$ for tag $M$ generating the word $can$ is $6/6 = 1$, since every occurrence of the tag $M$ corresponds to the word $can$ in the training data.\n",
    "\n",
    "Full tables for the transition and emission probabilities are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counts\n",
    "bigram_tag_counts = defaultdict(lambda: defaultdict(int))\n",
    "unigram_tag_counts = defaultdict(int)\n",
    "tag_word_counts = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "for sentence in tagged_text:\n",
    "    for (w1, t1), (w2, t2) in list(zip(sentence, sentence[1:])):\n",
    "        bigram_tag_counts[t1][t2] += 1\n",
    "        unigram_tag_counts[t1] += 1\n",
    "    for w, t in sentence:\n",
    "        tag_word_counts[t][w] += 1\n",
    "        tag_counts[t] += 1\n",
    "\n",
    "# Generate transition and emission probabilities\n",
    "a = defaultdict(lambda: defaultdict(int))\n",
    "b = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for t1 in parts_of_speech:\n",
    "    for t2 in parts_of_speech:\n",
    "        a[t1][t2] = bigram_tag_counts[t1][t2] / unigram_tag_counts[t1]\n",
    "    for w1 in vocabulary.keys():\n",
    "        b[t1][w1] = tag_word_counts[t1][w1] / tag_counts[t1]\n",
    "\n",
    "# Print tables of probabilities\n",
    "\n",
    "print(\"Transition probabilities: a_ij\")\n",
    "print(f\"{' ':6}\", end=\"\")\n",
    "for t in parts_of_speech:\n",
    "    print(f\"{t:>6}\", end=\"\")\n",
    "print()\n",
    "for t1 in parts_of_speech:\n",
    "    print(f\"{t1:<6}\", end=\"\")\n",
    "    for t2 in parts_of_speech:\n",
    "        print(f\"{a[t1][t2]:>6.2f}\", end=\"\")\n",
    "    print(\"\")\n",
    " \n",
    "print(\"\\nEmission probabilities: b_i(w_j)\")\n",
    "print(f\"{' ':6}\", end=\"\")\n",
    "for w in vocabulary.keys():\n",
    "    print(f\"{w:>8}\", end=\"\")\n",
    "print()\n",
    "for t in parts_of_speech:\n",
    "    print(f\"{t:<6}\", end=\"\")\n",
    "    for w in vocabulary.keys():\n",
    "        print(f\"{b[t][w]:>8.2f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example HMM trellis\n",
    "\n",
    "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/hmm-figure.png\" width=\"75%\" align=right />\n",
    "\n",
    "Now consider the HMM generating the example sentence \"canners can canned fish\". The figure at right contains the _trellis_ for the sentence. The horizontal axis corresponds to the words in the sentence, one at a time. The vertical axis corresponds to the states of the HMM (that is, the parts of speech). The gray arrows that connect a tag on the left to a tag on the right correspond to the transition probabilities. The red arrows that connect a tag to a word directly below correspond to the emission probabilities.\n",
    "\n",
    "We've highlighted two paths through the trellis from the beginning to the end of the sentence, corresponding to different taggings of the sentence. You should download a copy for ease in doing the next few exercises.\n",
    "\n",
    "1. Mark on the figure the path corresponding to the majority bigram tagging.\n",
    "2. Mark on the figure next to the highlighted paths the associated probabilities. The gray arrows should be marked with a transition probability and the red arrows with an emission probability. The tables above should come in handy.\n",
    "3. Calculate the probability of the majority bigram tagging by multiplying together all of the probabilities along the path that you marked in step 1. Don't forget the emission probabilities.\n",
    "4. Calculate the probabilities for the two paths that we've marked in the figure. These turn out to be the two paths with the highest probability. \n",
    "5. Which tagging has the highest probability according to this HMM?\n",
    "\n",
    "You'll want to submit a digital copy of your marked-up figure, along with answering the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - From 3: what is the HMM probability of the majority bigram tagging?\n",
    "step_3_probability = \"your answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution - From 3: what is the HMM probability of the majority bigram tagging?\n",
    "step_3_probability = 0.0\n",
    "# The M -> A transition has probability 0, so regardless of what other \n",
    "# probabilities get multiplied in, the probability of the whole path \n",
    "# through the trellis is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - From 4: what is the HMM probability for the two highlighted paths?\n",
    "step_4_probabilities = [\"probability of lower probability path\",\n",
    "                        \"probability of higher probability path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution - From 4: what is the HMM probability for the two highlighted paths?\n",
    "step_4_probabilities = [1.00 * 0.80 * 0.32 * 0.31 * 1.00 * 0.33 * 0.10 * 0.55 * 0.59, # <bos> N M V N\n",
    "                        1.00 * 0.80 * 0.32 * 0.46 * 0.50 * 0.22 * 1.00 * 1.00 * 0.59  # <bos> N V A N\n",
    "                       ]\n",
    "step_4_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - From 5: which tagging has the highest probability? Give your answer\n",
    "#        as a string as in the example below.\n",
    "step_5_highest = \"<bos> R V M A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution - From 5: which tagging has the highest probability?\n",
    "step_5_highest = \"<bos> N V A N\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection, what is the accuracy of the best HMM labeling, given as a proportion of the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "example_maj_bigram_label_accuracy = \"your answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "example_maj_bigram_label_accuracy = 5/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the highest probability tagging - The Viterbi algorithm\n",
    "\n",
    "Above, we merely asserted that the two highlighted paths are the two most probable, so that it was a simple matter to find the highest probability tagging by just comparing the probabilities of those two. But in general there can be a huge number of paths through a trellis such as this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are $N$ tags and a sentence of length $M$, how many paths through the HMM trellis will there be (using big-O notation)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "open_response_1 = \"your answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "open_response_1 = \"\"\"\n",
    "   At each of the M words there are N possibilities, \n",
    "   so the total is O(N^M), exponential in the sentence length.\n",
    "   That's bad.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Viterbi algorithm, named after famed electrical engineer [Andrew Viterbi](https://en.wikipedia.org/wiki/Andrew_Viterbi), is an efficient dynamic programming algorithm for performing this (otherwise impractical) computation. We'll do the first few steps of the Viterbi algorithm for the example here.\n",
    "\n",
    "Given a string of words $\\vect{x} = \\langle x_0, x_1, \\ldots, x_M \\rangle$ and a set of states (tags) $\\vect{q} = \\{q_0, q_1, \\ldots, q_N\\}$, the algorithm works by calculating a series of values $v_i(j)$ where $i$ ranges over the words in the sentence from $1$ to $M$ and $j$ ranges over the tags from $1$ to $N$. For simplicity, we'll assume an extra word and tag at the begining of the sentence, as above, so $x_0 = \\texttt{<bos>}$ and $q_0 = \\texttt{<bos>}$.\n",
    "The definition for $v$ then is:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "  v_0(0) &= 1 \\\\\n",
    "  v_0(j) &= 0  &\\mbox{for $j > 0$} \\\\\n",
    "  v_i(j) &= \\max_{j'=1}^N v_{i-1}(j') \\cdot a_{j' j} \\cdot b_{j}(x_i)\n",
    "         & \\mbox{for $i > 0$}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sample sentence above (canners can canned fish), calculate (at least) the first two \"layers\" of the Viterbi algorithm, that is, $v_0$ and $v_1$, filling in the table below. (We've filled in the zero-th layer for you already, as per the first two lines in the definition of the Viterbi calculation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TODO-->\n",
    "|   | tag     | v_0  \\<bos\\> | v_1 canners | v_2 can | v_3 canned | v_4 fish |\n",
    "|---|---------|--------------|-------------|---------|------------|----------|\n",
    "| 0 | \\<bos\\> |    1         |             |         |            |          |\n",
    "| 1 | N       |    0         |             |         |            |          |\n",
    "| 2 | V       |    0         |             |         |            |          |\n",
    "| 3 | M       |    0         |             |         |            |          |\n",
    "| 4 | P       |    0         |             |         |            |          |\n",
    "| 5 | A       |    0         |             |         |            |          |\n",
    "| 6 | R       |    0         |             |         |            |          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this calculation by hand is painful, but it should make clear what's going on. At each point $v_i(j)$ in the table, we've calculated the probability of the best path through the trellis from the beginning of the sentence to the current word  $x_i$, starting in the start state and ending in the current state $q_j$. To get the maximum probability of all paths in the trellis for the full sentence ending in any state, we merely look up the maximum value of $v_M(j)$.\n",
    "\n",
    "What is the complexity of filling in all of the entries in the Viterbi table? How does that compare with the complexity of the total number of paths through the trellis that you calculated above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "open_response_2 = \"your answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "open_response_2 = \"\"\"\n",
    "    There are $O(NM)$ entries in the table, and each requires a calculation\n",
    "    that takes a max over $N$ constant time calculations, so the total\n",
    "    complexity is $O(N^2M)$. This is quadratic in $N$ rather than exponential, \n",
    "    and linear in $M$, far better than the brute force method of enumerating \n",
    "    all paths.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(\"lab2-3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
